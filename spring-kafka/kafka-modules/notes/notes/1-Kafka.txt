Kafka:
------------------------------------------------------------------------------
	Distributed commit log as events happen in a micro service application, these micro service applications put these events onto a log apachec kafka which is a system,
	managing these logs.
	The famous and popular term for these logs is a topic.
	
	Kafka stores the events in a orderly fashion, and it also it write those to a disc not just one disk, it can replicate them across disks to ensure that the 
	messages or events are not lost.
	
	Microservices applications exchange events through these topics or streams in real time
	
	And since the data and event can be produced as soon as they are produced, we can have realtime analytics.
	
	We can do recommendations and make decisions based on these analytics.
	
	These micro services applications will have their own processing logic, they dont just read the events from the topic and send them to another topic
	
	They will define their own computional logic. This is where kafka comes with the streaming api.
	
	The micro service application need to group data, aggregate them, filter them, join them etc.,
	
	kafka gives us the streaming api, which is simple to use and will be able to do all this right out of the box in our micro services is using the kafka streaming api.
	
	If there is data related to our  applications in external databases or other systems, we can use kafka connect, 
	which can be easily configured open source kafka connectors to fetch and integrate with the other datasources or databases 
	to fetch the data into kafka or to send the data from kafka to these external sources in declarative manner


Advantages:
------------------------------------------------------------------------------
	Kafka supports multiple producers and consumers.
	
	Multiple producers can write to a single topic at the same time and multiple consumers can subscribe and consume the messages from the very same topic.
	
	Unlike traditional messaging systems, where once a message is consumed by a consumer from the topic, it will be gone.
	
	But in kafka it retains the message and another consumer application can still get the very same message.

	Kafka also supports consumer groups and partitions that is within a application, a consumer application 
	We can have parallel consumers running, which can a part of group called consumer group and the topic is divided into multiple partitions.
	
	THe messages from the producer will go across these partitions so that parallel processing is possible
	
	kafka ensures that within a consumer group, the message will be consumed only once. That is only one consumer within the consumer group will consume a 
	particular message. THere will be no duplication
	
	So across applications, it allows the possibility of, these applications processing the very same message.
	
	But within a application, it allows parallelism by having a consumer group, and these consumers can consume messages from various partitions 
	which are nothing but divisions within a topic.
	
	
	
	Secondly, it maintains a disk based persistence. That is even if a consumer is temporarily down becuase of restart or a crash, when it comes back up,
	the kafka broker would have retain the message in a disk storage with a preconfigurable amount of time, the consumer will get the message whenever it is ready.
	
	
	
	Third, scalability as the load on the kafka broker increases, theser brokers can be easily scaled, mostly for dev environment, we can have for 10 or more brokers.
	For production environment, we can have a  cluster of hundreds or thousands of  brokers.
	
	If one broker goes down, then another broker will take over, giving us availability as well.
	
	
Usecase:
------------------------------------------------------------------------------
	Kafka can be used to exchange messages	across micro service applications.
	This could be one micro service application sending notifications to a kafka topic to which other micro service application will subscribe and 
	consume the notification to take the appropriate action.
	
	We can also implement a simple producer consumer pattern where one micro service application is producing lot of work and other micro service applications consume this work
	and get it done like order shipping processing, etc.
	
	Activity tracking, this is how kafka has originated at linkedin, we can use it track it to track user activity or any activity, we can feedit to a topic which cane be 
	read or are consumed by a machine learning algorithm which can generate the recommendations for that user, the producer application or website the user isbrowsing  can read
	those recommendations and push them to the user whenever he does search. 
	EX:Amazon, Netflix
	
	Metrics and log aggregation as applications produce metrics on a regular basis or even logs, all those can be aggregated and stored in a permanent storage for analysis.
	Instead of waiting for along time as the applications are producing metrics and logs, they can be aggregated and put into systems like elastic search.
	And  we can right away analyze that data to see if there are any unwanted activities or security threats.

	Commit Log - use kafka as a commit log, as database changes happen, those changes can be streamed to a kafka topic andwe can use that data  to replicate the database if
	required, or simply to analyze what is going on with the database and see if there are many threats or unwanted activity
	
	Stream processing - all the above mentioned use cases canbe treated as streams, kafka streams open up doors for numerous use cases where we can create data pipelines 
	as the data flows through these pipelines and different stage will have transformation, computational logic, the data will be transformed at each step and big data tools
	like Hadoop, Storm etc., can be a part of this data pipelines as well.
	
	
Where:
------------------------------------------------------------------------------
Monitoring Trucks, Cars Shipments:
	Kafka is the foundation for various big data platforms and even driven micro service applications. Car, trucks and shipments can be tracked in real time using kafka.

Factories:
	In factories, kafka canbe used to capture and analyze the IOT device data and ake decisions onthe fly.

Financial Transactions:
		Process banking and stock exchange transactions in the financial world

Hospitals:
	The patient information can be tracked and monitored and any changes in his condition can also be monitored.
	


	
Kafka Architecture:
------------------------------------------------------------------------------
	Broker, Zookeeper, Producers, Consumers
	
Broker:	
	THe kafka cluster is a collection of kafka brokers, also referred as Kafka servers or Kafka nodes.
	
	It is through the broker that the messages are exchanged between the producer and consumer applications.
	THe broker not only decouples the producer and consumer, but it is also ensures that the messages are persisted and durable.
	
	The kafka broker is a java process and we can increase the number of brokers to provide scalability and durability of messages.
	
	One of the broker will be elected as a cluster leader or cluster controller. All other brokers will follow this leader.
	
	It is the responsibility of the cluster leader to manage the partitions, replicas and other adminstrative operations.
	

Zookeeper:
	Zookeeper is responsible for electing a cluster controller or leader, all the broker nodes will register themselves with the zookeeper component when they come up
	and the zookeeper will pick only one of brokers as the cluster leader.
	
	
	If that broker goes down, it will pick up another broker as the cluster leader.
	
	It also maintains the state of the cluster like the metadata of the leader and followers.
	
	The states, quotas, the access control list eveerything will be maintained by zookeeper
	
	zookeeper is apache opensopurce project of its own, when we install kafka, it will come bundled with zookeeper
	
	kafka also working on a component of its own that is going to replace zookeeper in the future.
	
	
Producer:
	Producer is an application that produces data that communicate with the cluster using TCP protocol and they connect with the broadcast directly and 
	start sending messages.
	
	A producer can send messages to multiple topics and a topic can receive messages from multiple producers.
	

Consumer:
	It is an application that consumes from one or more topics, consumes records or datafrom one or more topics and process it.
	
	Consumers co-ordinate among themselves as a group to balance the load and also track each others.
	
	

Record:
------------------------------------------------------------------------------	
	There are several attributes to a record, starting with Topic, Partition, Offset, Timestamp, Key, Headers, Value.

Topic:
	The topic is the topic to which this record should be returned to. 

Partition:	
	The partition is a zero based index to which the record should be written.
	A record is always associated with only one partition, and the partition number can be set by the producer application.
	And if it is not set, it will be calculated based on the key that is provided in the record.
	A hashvalue will be calculated by using the key value and the result will be used as the partition number  to which that record should go to.
	
Offset:
	It is a 64 bit signed integer for locating the record within a partition.
	
Timestamp:
	It is set by the producer application and if it is not set, then the producer API internally assigns the current time as the timestamp.
	
Key:
	Although it is called a key, it is an optional non unique value, it is array of bytes.
	A key value you present will be used to calculate the partition number. A hash algorithm will be used along with this.
	
	The value of the key and the partition number will be calculated to which the record should go to.
	
	If the key value is not set becuase it is optional, then the kafka will decide the  partition number in a round fashion.
	That is, it will assign the record to a particular partition based on the round robin fashion.
	
	This is not a primary key and we can even set the same key for multiple records on the producer, and all of those records will end up in the same partition.
	So we can control that using the value of the key.
	
	It is not  unique. If you want to uniquely identify a record, then you can use the partition number and the offset as using these two, you can 
	uniquely identify a record.
	
Headers:
	It is optional key value pairs, just like the http headers to pass in metadata.
	
Value:
	This is where the payload of message lies. It is an arrya of bytes.
	It is the one that  contains our business data, although the value is optional.
	Without it, the Record does not make any sense.
	
	So all the attributes are like metadata for this data we are exchanging using the value  attribute.
	
https://kafka.apache.org/23/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html

	
	
	
Topics Partitions and Offsets:
------------------------------------------------------------------------------
					Topic
	Partition 0		0|1|2|3|4|5|6|7|8|9|10
	Partition 1		0|1|2|3|4|5|6|7|8|9|10|11
	Partition 2		0|1|2|3|4|5|6|7|8|9|10|11
	Partition 3		0|1|2|3|4|5|6|7|8|9|10|11|12


	Each topic can be divided into one or more partitions.
	If kafka is a distributed messaging or commit log, then a partition is a single log of messages or Records.
	
	Messages are appended to the end of a partition as they come in.
	
	Kafka assigns  each partition a unique number and also each message or a record that is stored within a partition gets the offset value.
	
	This offset values like array index. It starts with zero for every partition.
	
	And as the messages are stored, it will be incremented the partition number and the offset value uniquely identify a record and also kafka stores the partition number
	and the offset value inside the record or message.
	
	Since messages can go across partitions, the messaging order across partitions is not guaranteed whereas the messaging order within a partition is maintained, the
	producer application can specify which partition the message should go into using the partition number.
	
	When a producer creates a record using the partition number attribute, it can tell which exact partition it should go into. Kafka will take the partition number and 
	put the message into that partition. If not, it can give you a key. kafka willuse this key to calculate a hash and whatever the hash is that will become the partition
	number into which that record will go into.
	Even if the key is not provided by the producer, then kafka will put the message into one of the partitions using round robin algorithm.
	
	Partitions give kafka the power of scalability and availability. That is these partitions can be scaled across brokers as the load increases, as there are too many
	messages coming in.
	
	
	Broker 0				Broker 1				Broker 2
	-------------------------------------------------------------
	Partition 0				Partition 2				Partition 3
	Partition 1
	
	instead of puting all the partitions in One Broker, we can scale them across Brokers here.
	Here there are four partitions on three brokers.
	
	This will increase the performance and the application can be easily scaled.
	
	
Replication or duplication:
	Partitions also support replication or duplication, which give high availability
	
	
	Broker 0				Broker 1				Broker 2
	-------------------------------------------------------------
	Partition 0				Partition 2				Partition 3
	Partition 1				Partition 0				Partition 2
	Partition 3				Partition 1			

	Here the partition zero and one are present in Broker zero as well as Broker one.
	The partition two is present in broker one and also broker two.
	And partition three is duplicated both in broker two and broker zero.
	
	This gives high availability. Even one broker goes down, another broker can take over.
	That does not mean that all the brokers are processing the messages from each of these partitions.
	It uses the concept of leader and follower here. Each partition will have a leader and follower here.
	
	Broker 0				Broker 1				Broker 2
	---------------------------------------------------------------
	Partition 0	(L)			Partition 2	(L)			Partition 3	(L)
	Partition 1	(L)			Partition 0	(F)			Partition 2 (F)
	Partition 3	(F)			Partition 1	(F)	
	
	
	if any of the leader goes down, the new leader will get elected from one of the followers.
	
	We have a replica and if something goes wrong, the other broker can take over that network.
	
Replication factor:	
	Duplication are the replicas we want using the replication factor, if the replciation factor is one, then there will be no duplication.
	If the replication factor is two, the partition will be duplicated, one copy will be created and they will be stored across brokers.
	If the replication factor is three, two copies will be created, totally 3 partitions will be there across brokers including the original
	
	
Consumer Groups:	
------------------------------------------------------------------------------
	Partitions bring concurrency to Kafka, that is by having messages that belong to the topic spread across partitions, we can create a consumer group.
	
	A consumer group is a set of consumers working together to consume a topic. A consumer group ensures that each partition is comsumed by only one consumer.


					Topic									Consumer group
					
	Partition 0		0|1|2|3|4|5|6|7|8|9|10			<------ Consumer 0
	
	Partition 1		0|1|2|3|4|5|6|7|8|9|10|11		<------ Consumer 1
	
	Partition 2		0|1|2|3|4|5|6|7|8|9|10|11		<------ Consumer 2
	
	Partition 3		0|1|2|3|4|5|6|7|8|9|10|11|12	<------ Consumer 2
	
	
	The first two consumers are consuming the first two partitions respectively, and the last consumer two is consuming the last two partitions.
	
	Assigning a consumer to a partition is called the ownership of the partition by the consumer.
	
	Consumers can easily be horizontally scalled. That is, if the load on a consumer is increasing, we scale up the consumers and the new consumer will take over a partition.
	
	Also if one consumers feels the remaining consumers can co-ordinate among themselves and take over the partitions that the failed consumer was working with.
	
	
					Topic									Consumer group
					
	Partition 0		0|1|2|3|4|5|6|7|8|9|10			<------ Consumer 0
	
	Partition 1		0|1|2|3|4|5|6|7|8|9|10|11		<------ Consumer 1
	
	Partition 2		0|1|2|3|4|5|6|7|8|9|10|11		<------ Consumer 2
	
	Partition 3		0|1|2|3|4|5|6|7|8|9|10|11|12	<------ Consumer 2
															
															Consumer 3
	
	
------------------------------------------------------------------------------


	
	
Batches:
	Producers will not send one message at a time to kafka broker. Instead they batch them based on topic and partition.
	The batch is a collection of messages that should be returned to the same topic and partition.
	This will reduce the network roundtrips which otherwise the each messages are required send to kafka broker individually.
	
	The larger the batch size, the more messages are processed in a given time frame.
	
	Batches are also compressed providing more efficient data transfer and storage. ofcourse it takes some time to do the compression.
	we can configure the batch size.
	
								Topic1,Partition0
								--------------------
								Batch0, Batch1, Batch2
kafka producer	->								
								Topic2,Partition1
								--------------------
								Batch0, Batch1, Batch2
	



Admin API:
												->	Topics
		KafkaDrop		->		Kafka Broker	->	Brokers
												-> Other Objects
												
Producer API:
	Producer api sends stream of messages to broker and unlike traditional messaging product, it does serialization, partition assignment
	
Consumer API:
	Consumer api consumes the messages from topic and it does Deserialization and Rebalancing
	

Streams API:
							kafka Cluster				Streams
Producer		->			Input Topic			->		Stream producers	-> Transform	-> Aggregate	-> Join
																												↓
																												
							output Topic																<-	Consumers

Connect API;
	Import and export data from external datasources

	
	
	
	
	
↑ ↓ → ←	

		
	
Set up Commands:
---------------
zookeeper-server-start <KAFKA_DIR>\zookeeper.properties

kafka-server-start <KAFKA_DIR>\server.properties
	
	

zookeeper-server-start F:\git\softwares\kafka\config\zookeeper.properties
kafka-server-start F:\git\softwares\kafka\config\server.properties

kafka-server-stop


Basic commands:
---------------	
	
kafka-topics --list --bootstrap-server localhost:9092

kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic first-topic

kafka-topics --describe --bootstrap-server localhost:9092 --topic first-topic

kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic

kafka-console-producer --broker-list localhost:9092 --topic first-topic

kafka-topics --delete --bootstrap-server localhost:9092 --topic first-topic                           

Note:

For delete add the following in kafka server.properties

delete.topic.enable=true 		-- server.properties


create partitions:
----------------
kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 10 --topic first-partitioned-topic
kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 10 --topic OrderPartitionedTopic
kafka-topics --describe --bootstrap-server localhost:9092 --topic first-partitioned-topic
kafka-topics --describe --bootstrap-server localhost:9092 --topic OrderPartitionedTopic

kafka-topics --delete --bootstrap-server localhost:9092 --topic OrderPartitionedTopic 


confluent:
--------
confluent local service start
confluent local service stop
	
	
	
Docker:
--------
docker run hello-world	
docker ps -a
	
	
	
	
	
Schema Compatability:
------------------------

As schema changes over a period of time, the consumer application might not always ready to use the latest schema and they dont aware of the changes done by the publishers,
where the schema Compatability in.

The producer takes the ownership of the schemas and when they change the schemas, they should ensure all the downstreams consumers are ok with the new schemas.

This is pretty difficult if there are large consumers.

That is where the schema registry allows you to configure the schema compatability rules at schema level and if the producer break the rules, they will immediately get exceptions.

Make sure the schemas are always backward compatability to do impact the consumers.


Poll:
-----
ConsumerRecords<String, Integer> orders = consumer.poll(Duration.ofSeconds(20));

consumer.poll - if there are rcords found during the duration, it will return it and close the consumer, if no records found, it will wait for the duration and then get closed.


Consumers run for infinite time and to runit infinite time, use some while loop with some condition
	
	
	
Various Configurations:
-------------------------

ProducerConfig.ACKS_CONFIG:
	The number of acknowledgments the producer requires the leader to have received before considering a request complete. 
	This controls the  durability of records that are sent. 
	The following settings are allowed:  
		acks=0	: 	If set to zero then the producer will not wait for any acknowledgment from the server at all. 
					The record will be immediately added to the socket buffer and considered sent. 
					No guarantee can be made that the server has received the record in this case, and the retries
					configuration will not take effect (as the client won't generally know of any failures). 
					The offset given back for each record will always be set to -1
					
		acks=1 :	This will mean the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers.
					In this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost. 
					
		acks=all:	This means the leader will wait for the full set of in-sync replicas to acknowledge the record. 
					This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. 
					This is the strongest available guarantee. This is equivalent to the acks=-1 setting.
					
					
				
ProducerConfig.BUFFER_MEMORY_CONFIG:
	Value is in bytes. Buffer memory the producer will use to buffer the messages before they are sent to the server.
	Default size is 256 MB.

	The total bytes of memory the producer can use to buffer records waiting to be sent to the server. 
	
	If records are sent faster than they can be delivered to the server the producer will block for <code>max.block.ms</code> after which it will throw an exception.
	<p>This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. 
	Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests


ProducerConfig.COMPRESSION_TYPE_CONFIG: snappy, gzip, lz4
	By default the messages are not compressed unti this is set.
	
	The compression type for all data generated by the producer. 
	The default is none (i.e. no compression).
	Valid  values are <code>none</code>, <code>gzip</code>, <code>snappy</code>, <code>lz4</code>, or <code>zstd</code>. 
	Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression)
	
	snappy from google uses les cpu
	gzip uses less network bandwidth and the compresson ratio is higher than snappy
	
ProducerConfig.RETRIES_CONFIG:
	Retry if any failure. The number of times to retry.  By defalt, it waits for 100 ms before it retries
	
ProducerConfig.RETRY_BACKOFF_MS_CONFIG:
	Wait for the milliseconds configured here to retry in case of failure


ProducerConfig.BATCH_SIZE_CONFIG:
	memory size to allocate for batch size.
	The producer handover the messages to sender thread, as soons as the sender thread is free and available to send the messages to broker.
	We can control this using below LINGER_MS_CONFIG.
	
ProducerConfig.LINGER_MS_CONFIG:
	property that compliments the above batch size.
	The producer waits for the configured milliseconds here, even the sender thread is available to take the messages to send.
	This will give the chance to fill the messages in batch size memory to increase the throughput
	
ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG:
	producer will wait for the response from broker and  timeout after.
	
	
	
props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.IntegerSerializer");
props.setProperty(ProducerConfig.ACKS_CONFIG, "all");
props.setProperty(ProducerConfig.BUFFER_MEMORY_CONFIG, "343434334");
props.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, "gzip");
props.setProperty(ProducerConfig.RETRIES_CONFIG, "2");
props.setProperty(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, "400");
props.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, "10243434343");
props.setProperty(ProducerConfig.LINGER_MS_CONFIG, "500");
props.setProperty(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, "200");	
	
	

Transactions:
--------------------------------
	Message Delivery Semantics:
		At least Once delivery
			
		At most once delivery
		Only once / Idempotency / Exactly once (No Duplication)
	
https://www.baeldung.com/kafka-message-delivery-semantics



void org.apache.kafka.clients.producer.KafkaProducer.initTransactions()
	Needs to be called before any other methods when the transactional.id is set in the configuration.
	This method does the following:
	1.  Ensures any transactions initiated by previous instances of the producer with the sametransactional.id are completed. 
		If the previous instance had failed with a transaction inprogress, it will be aborted. 
		If the last transaction had begun completion,but not yet finished, this method awaits its completion.
	2. 	Gets the internal producer id and epoch, used in all future transactionalmessages issued by the producer.
		Note that this method will raise TimeoutException if the transactional state cannotbe initialized before expiration of max.block.ms. 
		Additionally, it will raise InterruptExceptionif interrupted. It is safe to retry in either case, 
		but once the transactional state has been successfully initialized, this method should no longer be used.

	

void org.apache.kafka.clients.producer.KafkaProducer.beginTransaction() throws ProducerFencedException

	Should be called before the start of each new transaction.
	Note that prior to the first invocationof this method, you must invoke initTransactions() exactly one time.	
	

Future<RecordMetadata> org.apache.kafka.clients.producer.KafkaProducer.send(ProducerRecord<String, Integer> record, Callback callback)


	Asynchronously send a record to a topic and invoke the provided callback when the send has been acknowledged. 

	The send is asynchronous and this method will return immediately once the record has been stored in the buffer ofrecords waiting to be sent. 
	This allows sending many records in parallel without blocking to wait for theresponse after each one. 

	The result of the send is a RecordMetadata specifying the partition the record was sent to, the offsetit was assigned and the timestamp of the record. 
	If CreateTime is used by the topic, the timestampwill be the user provided timestamp or the record send time if the user did not specify a timestamp for therecord. 
	If LogAppendTime is used for thetopic, the timestamp will be the Kafka broker local time when the message is appended. 

	Since the send call is asynchronous it returns a Future for the RecordMetadata that will be assigned to this record. 
	Invoking get() on this future will block until the associated request completes and then return the metadata for the recordor 
	throw any exception that occurred while sending the record. 

	If you want to simulate a simple blocking call you can call the get() method immediately: 
	 
	 byte[] key = "key".getBytes();
	 byte[] value = "value".getBytes();
	 ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>("my-topic", key, value)
	 producer.send(record).get();

	Fully non-blocking usage can make use of the Callback parameter to provide a callback thatwill be invoked when the request is complete. 
	 
	 ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>("the-topic", key, value);
	 producer.send(myRecord,
				   new Callback() {
					   public void onCompletion(RecordMetadata metadata, Exception e) {
						   if(e != null) {
							  e.printStackTrace();
						   } else {
							  System.out.println("The offset of the record we just sent is: " + metadata.offset());
						   }
					   }
				   });
	 }
	 
	Callbacks for records being sent to the same partition are guaranteed to execute in order.
	That is, in thefollowing example callback1 is guaranteed to execute before callback2:  
	 producer.send(new ProducerRecord<byte[],byte[]>(topic, partition, key1, value1), callback1);
	 producer.send(new ProducerRecord<byte[],byte[]>(topic, partition, key2, value2), callback2);
	 

	When used as part of a transaction, it is not necessary to define a callback or check the result of the futurein order to detect errors from send.
	If any of the send calls failed with an irrecoverable error,the final commitTransaction() call will fail and throw the exception from the last failed send. 
	Whenthis happens, your application should call abortTransaction() to reset the state and continue to senddata. 

	Some transactional send errors cannot be resolved with a call to abortTransaction(). 
	In particular,if a transactional send finishes with a ProducerFencedException, a org.apache.kafka.common.errors.OutOfOrderSequenceException,
	a org.apache.kafka.common.errors.UnsupportedVersionException, or an org.apache.kafka.common.errors.AuthorizationException, 
	then the only option left is to call close().Fatal errors cause the producer to enter a defunct state in which future API calls will continue 
	to raisethe same underyling error wrapped in a new KafkaException. 

	It is a similar picture when idempotence is enabled, but no transactional.id has been configured.
	In this case, org.apache.kafka.common.errors.UnsupportedVersionException and org.apache.kafka.common.errors.AuthorizationException are considered fatal errors. 
	However, ProducerFencedException does not need to be handled. Additionally, 
	it is possible to continuesending after receiving an org.apache.kafka.common.errors.OutOfOrderSequenceException, 
	but doing socan result in out of order delivery of pending messages. To ensure proper ordering, you should close theproducer and create a new instance. 

	If the message format of the destination topic is not upgraded to 0.11.0.0, idempotent and transactionalproduce requests will fail 
	with an org.apache.kafka.common.errors.UnsupportedForMessageFormatExceptionerror. 
	If this is encountered during a transaction, it is possible to abort and continue.
	But note that futuresends to the same topic will continue receiving the same exception until the topic is upgraded. 

	Note that callbacks will generally execute in the I/O thread of the producer and so should be reasonably fast orthey will delay the sending of messages from other threads. 
	If you want to execute blocking or computationallyexpensive callbacks it is recommended to use your own java.util.concurrent.Executor in the callback body 
	to parallelize processing.

		
		

void org.apache.kafka.clients.producer.KafkaProducer.abortTransaction() throws ProducerFencedException
	Aborts the ongoing transaction. Any unflushed produce messages will be aborted when this call is made.
	This call will throw an exception immediately if any prior send(ProducerRecord) calls failed with a ProducerFencedException
	or an instance of org.apache.kafka.common.errors.AuthorizationException.Note that this method will raise TimeoutException 
	if the transaction cannot be aborted before expirationof max.block.ms. Additionally, it will raise InterruptException if interrupted.
	It is safe to retry in either case, but it is not possible to attempt a different operation (such as commitTransaction)
	since the abort may already be in the progress of completing. If not retrying, the only option is to close the producer.

			
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
		
	
		
	
	

