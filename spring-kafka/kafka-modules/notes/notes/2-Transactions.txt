
	
Schema Compatability:
------------------------

As schema changes over a period of time, the consumer application might not always ready to use the latest schema and they dont aware of the changes done by the publishers,
where the schema Compatability in.

The producer takes the ownership of the schemas and when they change the schemas, they should ensure all the downstreams consumers are ok with the new schemas.

This is pretty difficult if there are large consumers.

That is where the schema registry allows you to configure the schema compatability rules at schema level and if the producer break the rules, they will immediately get exceptions.

Make sure the schemas are always backward compatability to do impact the consumers.


Poll:
-----
ConsumerRecords<String, Integer> orders = consumer.poll(Duration.ofSeconds(20));

consumer.poll - if there are rcords found during the duration, it will return it and close the consumer, if no records found, it will wait for the duration and then get closed.


Consumers run for infinite time and to runit infinite time, use some while loop with some condition
	
	
	
Various Configurations:
-------------------------

ProducerConfig.ACKS_CONFIG:
	The number of acknowledgments the producer requires the leader to have received before considering a request complete. 
	This controls the  durability of records that are sent. 
	The following settings are allowed:  
		acks=0	: 	If set to zero then the producer will not wait for any acknowledgment from the server at all. 
					The record will be immediately added to the socket buffer and considered sent. 
					No guarantee can be made that the server has received the record in this case, and the retries
					configuration will not take effect (as the client won't generally know of any failures). 
					The offset given back for each record will always be set to -1
					
		acks=1 :	This will mean the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers.
					In this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost. 
					
		acks=all:	This means the leader will wait for the full set of in-sync replicas to acknowledge the record. 
					This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. 
					This is the strongest available guarantee. This is equivalent to the acks=-1 setting.
					
					
				
ProducerConfig.BUFFER_MEMORY_CONFIG:
	Value is in bytes. Buffer memory the producer will use to buffer the messages before they are sent to the server.
	Default size is 256 MB.

	The total bytes of memory the producer can use to buffer records waiting to be sent to the server. 
	
	If records are sent faster than they can be delivered to the server the producer will block for <code>max.block.ms</code> after which it will throw an exception.
	<p>This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. 
	Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests


ProducerConfig.COMPRESSION_TYPE_CONFIG: snappy, gzip, lz4
	By default the messages are not compressed unti this is set.
	
	The compression type for all data generated by the producer. 
	The default is none (i.e. no compression).
	Valid  values are <code>none</code>, <code>gzip</code>, <code>snappy</code>, <code>lz4</code>, or <code>zstd</code>. 
	Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression)
	
	snappy from google uses les cpu
	gzip uses less network bandwidth and the compresson ratio is higher than snappy
	
ProducerConfig.RETRIES_CONFIG:
	Retry if any failure. The number of times to retry.  By default, it waits for 100 ms before it retries
	
ProducerConfig.RETRY_BACKOFF_MS_CONFIG:
	Wait for the milliseconds configured here before retry in case of failure


ProducerConfig.BATCH_SIZE_CONFIG:
	memory size to allocate for batch size.
	The producer handover the messages to sender thread, as soons as the sender thread is free and available to send the messages to broker.
	We can control this using below LINGER_MS_CONFIG.
	
ProducerConfig.LINGER_MS_CONFIG:
	property that compliments the above batch size.
	The producer waits for the configured milliseconds here, even the sender thread is available to take the messages to send.
	This will give the chance to fill the messages in batch size memory to increase the throughput
	
ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG:
	producer will wait for the response from broker and  timeout after.
	
	
	
props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.IntegerSerializer");
props.setProperty(ProducerConfig.ACKS_CONFIG, "all");
props.setProperty(ProducerConfig.BUFFER_MEMORY_CONFIG, "343434334");
props.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, "gzip");
props.setProperty(ProducerConfig.RETRIES_CONFIG, "2");
props.setProperty(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, "400");
props.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, "10243434343");
props.setProperty(ProducerConfig.LINGER_MS_CONFIG, "500");
props.setProperty(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, "200");	
	
	

Transactions:
--------------------------------
	Message Delivery Semantics:
		At least Once delivery (Default)			
		At most once delivery
		Only once / Idempotency / Exactly once (No Duplication of messages)
	
		https://www.baeldung.com/kafka-message-delivery-semantics


At least Once delivery (Default):
----------------------------------------------------------------------------------------------------------------------------------------------------------------
	If the message is received by the broker once, everybody is happy
	The producer sends the message, the broker takes it and writes it to all the partitions.
	If have replications of partitions and once all the partitions, the leader, the follower recieves the message, the broker treats it as committed,
	at which point it sends an acknowledgment back to producer. The message is delivered atleast once.
	
	
	But when it is trying to send an acknowledgment to the producer, if the acknowledgment fails for some reason, the producer API will
	automatically retry to send very same message resulting in message duplication.
	
	
	If it is OK to have message duplication and  reprocessing of same message, then use this default behavior which is alteast once delivery and can be delivered more
	than once in case of acknowledgment failures or any other.
	
							Producer							
							  |  |
					  message1|	 | retry
							  ↓  ↓	
								
							Broker 0 (message1)		Broker 1				Broker 2
							---------------------------------------------------------------
							Partition 0	(L)			Partition 2	(L)			Partition 3	(L)
							Partition 1	(L)			Partition 0	(F)			Partition 2 (F)
							Partition 3	(F)			Partition 1	(F)	
At most once delivery
----------------------------------------------------------------------------------------------------------------------------------------------------------------
	The message is delivered atmost once and not more than that.
	
	We do that using the retries configuration. The producer will use the retry configuration and set it to zero. That is, the producer will never retry.
	The producer send a message, if the broker writes it to all  the partitions, then everybody is happy.
	But if  it doesnot write, we have a issue becuase the producer wont retry and the message is lost.
	
	
	In case of at most once delivery, since the producer doest not retry, we have a issue of messages getting lost.
	For this, simply comfiguring the retries set to zero.
	
 
							Producer (No retries)							
							  |  
					  message1|	 
							  ↓  
								
							Broker 0 (message1)		Broker 1				Broker 2
							---------------------------------------------------------------
							Partition 0	(L)			Partition 2	(L)			Partition 3	(L)
							Partition 1	(L)			Partition 0	(F)			Partition 2 (F)
							Partition 3	(F)			Partition 1	(F)	
												
						
Only once / Idempotency / Exactly once (No Duplication of messages)
----------------------------------------------------------------------------------------------------------------------------------------------------------------
	To use this, apply the producer configuration called enable.idempotence = true
	
	The producer send method will generate a unique sequence number for every message.
	So it is unique within a given partition, the broker will receive it and the broker will maintain the message sequence number.
	Along with it, the broker will generate a unique producer id for each producer instance which will be used by the producer in the message everytime it sends a message.
	
	so the broker ensures that if a particular producer with a unique producer id, is sending the same message again, because now it has a sequence number, per partition within
	a partition, there will be multiple sequence numbers.
	
	it will check that sequence number. And if it has already received a message with that sequence number within a given partition from a given producer with unique id,
	then it will reject that message.
	
	Here we have all the retires as well. Also at the same time, we are avoiding duplication also.
	
	
	if multiple instances of the same producer sends duplicate messages, then this does not work, the idempotency will fail.
	
	The broker works, withing a producer id or producer instance, it can ensure that no duplicate messageq happens.
	
	But if multiple producer instances running, then both will get unique producer id and if they both sends duplicate message,  
	then the broker will not know still the duplication can happen.
	
							Producer							
							  |  ↑
				  m1 (seqNum) |	 | producer id
							  ↓  |	
								
							Broker 0 				Broker 1				Broker 2
							---------------------------------------------------------------
							Partition 0	(L)			Partition 2	(L)			Partition 3	(L)
							m1 (seqNum)
							
							Partition 1	(L)			Partition 0	(F)			Partition 2 (F)
													m1 (seqNum)
							
							Partition 3	(F)			Partition 1	(F)	
	
	
void org.apache.kafka.clients.producer.KafkaProducer.initTransactions()
---------------------------------------------------------------
	Needs to be called before any other methods when the transactional.id is set in the configuration.
	This method does the following:
	1.  Ensures any transactions initiated by previous instances of the producer with the same transactional.id are completed. 
		If the previous instance had failed with a transaction inprogress, it will be aborted. 
		If the last transaction had begun completion,but not yet finished, this method awaits its completion.
	2. 	Gets the internal producer id and epoch, used in all future transactional messages issued by the producer.
		Note that this method will raise TimeoutException if the transactional state cannotbe initialized before expiration of max.block.ms. 
		Additionally, it will raise InterruptExceptionif interrupted. It is safe to retry in either case, 
		but once the transactional state has been successfully initialized, this method should no longer be used.

	

void org.apache.kafka.clients.producer.KafkaProducer.beginTransaction() throws ProducerFencedException
------------------------------------------------------------------------------------------------------------------------------

	Should be called before the start of each new transaction.
	Note that prior to the first invocationof this method, you must invoke initTransactions() exactly one time.	
	

Future<RecordMetadata> org.apache.kafka.clients.producer.KafkaProducer.send(ProducerRecord<String, Integer> record, Callback callback)
------------------------------------------------------------------------------------------------------------------------------

	Asynchronously send a record to a topic and invoke the provided callback when the send has been acknowledged. 

	The send is asynchronous and this method will return immediately once the record has been stored in the buffer ofrecords waiting to be sent. 
	This allows sending many records in parallel without blocking to wait for theresponse after each one. 

	The result of the send is a RecordMetadata specifying the partition the record was sent to, the offsetit was assigned and the timestamp of the record. 
	If CreateTime is used by the topic, the timestampwill be the user provided timestamp or the record send time if the user did not specify a timestamp for therecord. 
	If LogAppendTime is used for thetopic, the timestamp will be the Kafka broker local time when the message is appended. 

	Since the send call is asynchronous it returns a Future for the RecordMetadata that will be assigned to this record. 
	Invoking get() on this future will block until the associated request completes and then return the metadata for the recordor 
	throw any exception that occurred while sending the record. 

	If you want to simulate a simple blocking call you can call the get() method immediately: 
	 
	 byte[] key = "key".getBytes();
	 byte[] value = "value".getBytes();
	 ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>("my-topic", key, value)
	 producer.send(record).get();

	Fully non-blocking usage can make use of the Callback parameter to provide a callback thatwill be invoked when the request is complete. 
	 
	 ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>("the-topic", key, value);
	 producer.send(myRecord,
				   new Callback() {
					   public void onCompletion(RecordMetadata metadata, Exception e) {
						   if(e != null) {
							  e.printStackTrace();
						   } else {
							  System.out.println("The offset of the record we just sent is: " + metadata.offset());
						   }
					   }
				   });
	 }
	 
	Callbacks for records being sent to the same partition are guaranteed to execute in order.
	That is, in thefollowing example callback1 is guaranteed to execute before callback2:  
	 producer.send(new ProducerRecord<byte[],byte[]>(topic, partition, key1, value1), callback1);
	 producer.send(new ProducerRecord<byte[],byte[]>(topic, partition, key2, value2), callback2);
	 

	When used as part of a transaction, it is not necessary to define a callback or check the result of the futurein order to detect errors from send.
	If any of the send calls failed with an irrecoverable error,the final commitTransaction() call will fail and throw the exception from the last failed send. 
	Whenthis happens, your application should call abortTransaction() to reset the state and continue to senddata. 

	Some transactional send errors cannot be resolved with a call to abortTransaction(). 
	In particular,if a transactional send finishes with a ProducerFencedException, a org.apache.kafka.common.errors.OutOfOrderSequenceException,
	a org.apache.kafka.common.errors.UnsupportedVersionException, or an org.apache.kafka.common.errors.AuthorizationException, 
	then the only option left is to call close().Fatal errors cause the producer to enter a defunct state in which future API calls will continue 
	to raisethe same underyling error wrapped in a new KafkaException. 

	It is a similar picture when idempotence is enabled, but no transactional.id has been configured.
	In this case, org.apache.kafka.common.errors.UnsupportedVersionException and org.apache.kafka.common.errors.AuthorizationException are considered fatal errors. 
	However, ProducerFencedException does not need to be handled. Additionally, 
	it is possible to continuesending after receiving an org.apache.kafka.common.errors.OutOfOrderSequenceException, 
	but doing socan result in out of order delivery of pending messages. To ensure proper ordering, you should close theproducer and create a new instance. 

	If the message format of the destination topic is not upgraded to 0.11.0.0, idempotent and transactionalproduce requests will fail 
	with an org.apache.kafka.common.errors.UnsupportedForMessageFormatExceptionerror. 
	If this is encountered during a transaction, it is possible to abort and continue.
	But note that futuresends to the same topic will continue receiving the same exception until the topic is upgraded. 

	Note that callbacks will generally execute in the I/O thread of the producer and so should be reasonably fast orthey will delay the sending of messages from other threads. 
	If you want to execute blocking or computationallyexpensive callbacks it is recommended to use your own java.util.concurrent.Executor in the callback body 
	to parallelize processing.

		
		

void org.apache.kafka.clients.producer.KafkaProducer.abortTransaction() throws ProducerFencedException
------------------------------------------------------------------------------------------------------------------------------
	Aborts the ongoing transaction. Any unflushed produce messages will be aborted when this call is made.
	This call will throw an exception immediately if any prior send(ProducerRecord) calls failed with a ProducerFencedException
	or an instance of org.apache.kafka.common.errors.AuthorizationException.Note that this method will raise TimeoutException 
	if the transaction cannot be aborted before expirationof max.block.ms. Additionally, it will raise InterruptException if interrupted.
	It is safe to retry in either case, but it is not possible to attempt a different operation (such as commitTransaction)
	since the abort may already be in the progress of completing. If not retrying, the only option is to close the producer.

			
	
	
	
	
Summary:
	THe same producer instance cannot have multiple transactions open at the same time that it has to close or commit a transaction before it starts a another transaction.
	
	The commit transaction method flushes any unsent records before the commit happens.
	
	If there are any errors happened during the commit, it will a exception. That is the reason, we dont need to have this below callback.
	
		producer.send(record,new OrderCallback()); // Callback is not required  in transactions. If any failure, have to handle abort transactions to reset
		
		
	The producers are thread safe that is we can invoke the send method from multiple threads.
	But we have to make sure that we start the transaction before any of those threads starts and we commit the transaction after all the threads finished.
	So it is our responsibility to ensure that.
		
	

	
	
